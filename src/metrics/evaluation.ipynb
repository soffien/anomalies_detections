{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab856ced",
   "metadata": {},
   "source": [
    "# Dynamic Graph Anomaly Detection - Comprehensive Evaluation\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework for the DiGress-based anomaly detection system. It covers:\n",
    "\n",
    "1. **Model Performance Evaluation**\n",
    "2. **Reconstruction Error Analysis** \n",
    "3. **Threshold Selection & Validation**\n",
    "4. **Temporal Feature Importance**\n",
    "5. **Attention Mechanism Analysis**\n",
    "6. **Anomaly Detection Metrics**\n",
    "7. **Comparative Analysis**\n",
    "8. **Visualization & Interpretation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4ea591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/sofien/Desktop/anomalies_detections\n",
      "Source path: /home/sofien/Desktop/anomalies_detections/src\n",
      "Anomalies path: /home/sofien/Desktop/anomalies_detections/anomalies_detection\n",
      "‚úÖ Core models imported successfully!\n",
      "‚úÖ Anomaly detection functions imported successfully!\n",
      "üìä Evaluation framework loaded successfully!\n",
      "PyTorch version: 2.7.0+cu126\n",
      "Device available: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import stats\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix project paths - we need to go up to the project root\n",
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))  # Go up from src/metrics to project root\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "anomalies_path = os.path.join(project_root, 'anomalies_detection')\n",
    "\n",
    "sys.path.insert(0, project_root)\n",
    "sys.path.insert(0, src_path)\n",
    "sys.path.insert(0, anomalies_path)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Source path: {src_path}\")\n",
    "print(f\"Anomalies path: {anomalies_path}\")\n",
    "\n",
    "# Import project modules with corrected paths\n",
    "try:\n",
    "    from models.denoising_network import DenoisingNetwork\n",
    "    from models.transition_matrices import DiGressTransitionMatrices\n",
    "    from models.graph_features import compute_graph_features\n",
    "    from models.graph_temporel_features import compute_temporal_graph_features\n",
    "    from models.visualize_snapshot import SnapshotLoader, get_snapshot_files\n",
    "    print(\"‚úÖ Core models imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing core models: {e}\")\n",
    "\n",
    "try:\n",
    "    from run_anomaly_detection import (\n",
    "        load_trained_model_and_threshold, \n",
    "        compute_single_graph_error,\n",
    "        detect_anomaly,\n",
    "        detect_multiple_anomalies\n",
    "    )\n",
    "    print(\"‚úÖ Anomaly detection functions imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing anomaly detection functions: {e}\")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä Evaluation framework loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838087c8",
   "metadata": {},
   "source": [
    "## 1. Configuration & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2fd98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration:\n",
      "   Dataset: uci\n",
      "   Mode: dynamic\n",
      "   Model path: /home/sofien/Desktop/anomalies_detections/saved_models/digress_dynamic_final_uci.pt\n",
      "   Threshold path: /home/sofien/Desktop/anomalies_detections/saved_models/threshold_vector_dynamic_uci.pt\n",
      "üìÅ Total snapshots available: 0\n",
      "üéØ Test snapshots: 0\n",
      "üèãÔ∏è Train snapshots: 0\n",
      "‚ö†Ô∏è No test snapshots found\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'dataset': 'uci',\n",
    "    'mode': 'dynamic',\n",
    "    'device': torch.device('cpu'),\n",
    "    'num_test_samples': 100,\n",
    "    'random_seed': 42,\n",
    "    'model_path': os.path.join(project_root, 'saved_models', 'digress_dynamic_final_uci.pt'),\n",
    "    'threshold_path': os.path.join(project_root, 'saved_models', 'threshold_vector_dynamic_uci.pt')\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "print(f\"üìÅ Configuration:\")\n",
    "print(f\"   Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"   Mode: {CONFIG['mode']}\")\n",
    "print(f\"   Model path: {CONFIG['model_path']}\")\n",
    "print(f\"   Threshold path: {CONFIG['threshold_path']}\")\n",
    "\n",
    "# Load available snapshots\n",
    "try:\n",
    "    all_snapshots = get_snapshot_files(CONFIG['dataset'])\n",
    "    print(f\"üìÅ Total snapshots available: {len(all_snapshots)}\")\n",
    "    \n",
    "    # Sample for evaluation (last 20% as test set)\n",
    "    test_size = int(len(all_snapshots) * 0.2)\n",
    "    test_snapshots = all_snapshots[-test_size:]\n",
    "    train_snapshots = all_snapshots[:-test_size]\n",
    "\n",
    "    print(f\"üéØ Test snapshots: {len(test_snapshots)}\")\n",
    "    print(f\"üèãÔ∏è Train snapshots: {len(train_snapshots)}\")\n",
    "\n",
    "    # Display sample snapshot info\n",
    "    if test_snapshots:\n",
    "        sample_snapshot = test_snapshots[0]\n",
    "        X_sample = SnapshotLoader.get_X(sample_snapshot)\n",
    "        E_sample = SnapshotLoader.get_E(sample_snapshot)\n",
    "        print(f\"\\nüìä Sample snapshot dimensions:\")\n",
    "        print(f\"   X (nodes): {X_sample.shape}\")\n",
    "        print(f\"   E (edges): {E_sample.shape}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No test snapshots found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading snapshots: {e}\")\n",
    "    print(\"Please ensure you have generated snapshots by running the data preparation scripts first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a087b",
   "metadata": {},
   "source": [
    "## 2. Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6029e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement du mod√®le et seuil pour uci (mode: dynamic)\n",
      "‚ùå Error loading model:  Mod√®le non trouv√©: saved_models/digress_dynamic_final_uci.pt\n",
      " Ex√©cutez d'abord main.py pour entra√Æner le mod√®le\n",
      "Please ensure you have run main.py to train the model first!\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and threshold vector\n",
    "try:\n",
    "    model, transition_matrices, threshold_vector = load_trained_model_and_threshold(\n",
    "        dataset=CONFIG['dataset'], \n",
    "        mode=CONFIG['mode']\n",
    "    )\n",
    "    print(\"‚úÖ Model and threshold loaded successfully!\")\n",
    "    \n",
    "    # Display model information\n",
    "    print(f\"\\nüîß Model Configuration:\")\n",
    "    print(f\"   Mode: {CONFIG['mode']}\")\n",
    "    print(f\"   Device: {CONFIG['device']}\")\n",
    "    \n",
    "    # Display threshold information\n",
    "    print(f\"\\nüìä Threshold Vector:\")\n",
    "    for key, value in threshold_vector.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"   {key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Please ensure you have run main.py to train the model first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ca62f",
   "metadata": {},
   "source": [
    "## 3. Reconstruction Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2792e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Computing reconstruction errors for test set...\n",
      "‚úÖ Computed 0 reconstruction errors\n",
      "\n",
      "üìä Reconstruction Error Statistics:\n",
      "   Mean: nan\n",
      "   Std: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(reconstruction_errors)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.std(reconstruction_errors)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstruction_errors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Max: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.max(reconstruction_errors)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Median: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.median(reconstruction_errors)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/anomalies_detections/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3302\u001b[39m, in \u001b[36mmin\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_min_dispatcher)\n\u001b[32m   3191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   3192\u001b[39m         where=np._NoValue):\n\u001b[32m   3193\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3194\u001b[39m \u001b[33;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[32m   3195\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3300\u001b[39m \u001b[33;03m    6\u001b[39;00m\n\u001b[32m   3301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3303\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/anomalies_detections/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# Compute reconstruction errors for test set\n",
    "print(\"üîÑ Computing reconstruction errors for test set...\")\n",
    "\n",
    "reconstruction_errors = []\n",
    "test_samples = test_snapshots[:CONFIG['num_test_samples']]  # Limit for faster evaluation\n",
    "\n",
    "for i, snapshot_path in enumerate(test_samples):\n",
    "    try:\n",
    "        # Load snapshot\n",
    "        X = SnapshotLoader.get_X(snapshot_path)\n",
    "        E = SnapshotLoader.get_E(snapshot_path)\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        error = compute_single_graph_error(X, E, model, transition_matrices, CONFIG['mode'])\n",
    "        reconstruction_errors.append(error)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"   Processed {i+1}/{len(test_samples)} samples...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error processing sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "reconstruction_errors = np.array(reconstruction_errors)\n",
    "print(f\"‚úÖ Computed {len(reconstruction_errors)} reconstruction errors\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìä Reconstruction Error Statistics:\")\n",
    "print(f\"   Mean: {np.mean(reconstruction_errors):.6f}\")\n",
    "print(f\"   Std: {np.std(reconstruction_errors):.6f}\")\n",
    "print(f\"   Min: {np.min(reconstruction_errors):.6f}\")\n",
    "print(f\"   Max: {np.max(reconstruction_errors):.6f}\")\n",
    "print(f\"   Median: {np.median(reconstruction_errors):.6f}\")\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "print(f\"\\nüìà Percentiles:\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(reconstruction_errors, p)\n",
    "    print(f\"   {p}th percentile: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b71319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction error distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(reconstruction_errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(threshold_vector['percentile_95_threshold'], color='red', linestyle='--', \n",
    "                   label=f\"95% Threshold: {threshold_vector['percentile_95_threshold']:.4f}\")\n",
    "axes[0, 0].axvline(np.mean(reconstruction_errors), color='green', linestyle='--', \n",
    "                   label=f\"Mean: {np.mean(reconstruction_errors):.4f}\")\n",
    "axes[0, 0].set_title('Reconstruction Error Distribution')\n",
    "axes[0, 0].set_xlabel('Reconstruction Error')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(reconstruction_errors, vert=True)\n",
    "axes[0, 1].set_title('Reconstruction Error Box Plot')\n",
    "axes[0, 1].set_ylabel('Reconstruction Error')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot (normality test)\n",
    "stats.probplot(reconstruction_errors, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Normality Test)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-scale distribution\n",
    "axes[1, 1].hist(np.log1p(reconstruction_errors), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Log-Scale Error Distribution')\n",
    "axes[1, 1].set_xlabel('Log(1 + Reconstruction Error)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "shapiro_stat, shapiro_p = stats.shapiro(reconstruction_errors[:min(5000, len(reconstruction_errors))])\n",
    "print(f\"\\nüß™ Statistical Tests:\")\n",
    "print(f\"   Shapiro-Wilk test (normality): statistic={shapiro_stat:.4f}, p-value={shapiro_p:.6f}\")\n",
    "print(f\"   Distribution appears {'normal' if shapiro_p > 0.05 else 'non-normal'} (Œ±=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b065dd",
   "metadata": {},
   "source": [
    "## 4. Threshold Selection & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3abebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different threshold strategies\n",
    "threshold_strategies = {\n",
    "    'percentile_95': np.percentile(reconstruction_errors, 95),\n",
    "    'percentile_99': np.percentile(reconstruction_errors, 99),\n",
    "    'mean_plus_2std': np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors),\n",
    "    'mean_plus_3std': np.mean(reconstruction_errors) + 3 * np.std(reconstruction_errors),\n",
    "    'iqr_outlier': np.percentile(reconstruction_errors, 75) + 1.5 * (np.percentile(reconstruction_errors, 75) - np.percentile(reconstruction_errors, 25))\n",
    "}\n",
    "\n",
    "# Compare with stored thresholds\n",
    "print(\"üéØ Threshold Comparison:\")\n",
    "print(f\"{'Strategy':<20} {'Computed':<12} {'Stored':<12} {'Difference':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for strategy, computed_value in threshold_strategies.items():\n",
    "    stored_key = f\"{strategy}_threshold\"\n",
    "    stored_value = threshold_vector.get(stored_key, \"N/A\")\n",
    "    \n",
    "    if isinstance(stored_value, (int, float)):\n",
    "        diff = abs(computed_value - stored_value)\n",
    "        print(f\"{strategy:<20} {computed_value:<12.6f} {stored_value:<12.6f} {diff:<12.6f}\")\n",
    "    else:\n",
    "        print(f\"{strategy:<20} {computed_value:<12.6f} {'N/A':<12} {'N/A':<12}\")\n",
    "\n",
    "# Analyze anomaly detection rates for different thresholds\n",
    "print(f\"\\nüìä Anomaly Detection Rates:\")\n",
    "print(f\"{'Threshold':<20} {'Value':<12} {'Anomaly Rate':<12} {'Count':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, threshold_val in threshold_strategies.items():\n",
    "    anomalies = reconstruction_errors > threshold_val\n",
    "    anomaly_rate = np.mean(anomalies)\n",
    "    anomaly_count = np.sum(anomalies)\n",
    "    print(f\"{name:<20} {threshold_val:<12.6f} {anomaly_rate:<12.3%} {anomaly_count:<8}\")\n",
    "\n",
    "# ROC-like analysis (assuming last 10% as anomalies for evaluation)\n",
    "n_true_anomalies = int(len(reconstruction_errors) * 0.1)\n",
    "true_anomaly_indices = np.argsort(reconstruction_errors)[-n_true_anomalies:]\n",
    "true_labels = np.zeros(len(reconstruction_errors))\n",
    "true_labels[true_anomaly_indices] = 1\n",
    "\n",
    "# Compute metrics for different thresholds\n",
    "thresholds_to_test = np.percentile(reconstruction_errors, np.linspace(50, 99.9, 50))\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    predicted = reconstruction_errors > thresh\n",
    "    \n",
    "    if np.sum(predicted) > 0:\n",
    "        precision = np.sum(predicted & (true_labels == 1)) / np.sum(predicted)\n",
    "        recall = np.sum(predicted & (true_labels == 1)) / np.sum(true_labels)\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    else:\n",
    "        precision = recall = f1 = 0\n",
    "    \n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds_to_test[optimal_idx]\n",
    "optimal_f1 = f1_scores[optimal_idx]\n",
    "\n",
    "print(f\"\\nüéØ Optimal Threshold Analysis (assuming top 10% as anomalies):\")\n",
    "print(f\"   Optimal threshold: {optimal_threshold:.6f}\")\n",
    "print(f\"   Optimal F1-score: {optimal_f1:.4f}\")\n",
    "print(f\"   Precision at optimal: {precision_scores[optimal_idx]:.4f}\")\n",
    "print(f\"   Recall at optimal: {recall_scores[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2950903",
   "metadata": {},
   "source": [
    "## 5. Temporal Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d973a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze temporal features from a sample of graphs\n",
    "print(\"üîç Analyzing temporal features...\")\n",
    "\n",
    "# Feature names corresponding to the 22 temporal features\n",
    "TEMPORAL_FEATURE_NAMES = [\n",
    "    't_norm', 'seq_position', 'edge_birth_rate', 'edge_death_rate', 'edge_stability',\n",
    "    'edge_intermittency', 'degree_evolution_rate', 'degree_evolution_volatility', \n",
    "    'density_evolution', 'neighborhood_stability', 'clustering_evolution',\n",
    "    'triangle_evolution', 'structural_autocorr', 'structural_periodicity',\n",
    "    'centrality_evolution_mean', 'centrality_evolution_variance', 'centrality_persistence',\n",
    "    'structural_entropy', 'structural_surprise', 'edge_predictability',\n",
    "    'global_volatility', 'temporal_trend'\n",
    "]\n",
    "\n",
    "# Collect temporal features from sample snapshots\n",
    "temporal_features_matrix = []\n",
    "sample_size = min(50, len(test_samples))  # Use subset for faster computation\n",
    "\n",
    "for i in range(sample_size):\n",
    "    try:\n",
    "        snapshot_path = test_samples[i]\n",
    "        X = SnapshotLoader.get_X(snapshot_path)\n",
    "        E = SnapshotLoader.get_E(snapshot_path)\n",
    "        \n",
    "        # Create a simple graph sequence for temporal feature computation\n",
    "        class SimpleGraph:\n",
    "            def __init__(self, X, E):\n",
    "                self.X = X\n",
    "                self.E = E\n",
    "        \n",
    "        graph = SimpleGraph(X, E)\n",
    "        \n",
    "        # Compute temporal features\n",
    "        features = compute_temporal_graph_features(\n",
    "            [graph] * 3,  # Simple sequence\n",
    "            current_t=500,  # Middle timestep\n",
    "            num_timesteps=1000,\n",
    "            device=CONFIG['device'],\n",
    "            window_size=3\n",
    "        )\n",
    "        \n",
    "        temporal_features_matrix.append(features.cpu().numpy())\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"   Extracted features from {i+1}/{sample_size} samples...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error extracting features from sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "temporal_features_matrix = np.array(temporal_features_matrix)\n",
    "print(f\"‚úÖ Extracted temporal features: {temporal_features_matrix.shape}\")\n",
    "\n",
    "# Analyze feature statistics\n",
    "feature_stats = pd.DataFrame({\n",
    "    'Feature': TEMPORAL_FEATURE_NAMES[:temporal_features_matrix.shape[1]],\n",
    "    'Mean': np.mean(temporal_features_matrix, axis=0),\n",
    "    'Std': np.std(temporal_features_matrix, axis=0),\n",
    "    'Min': np.min(temporal_features_matrix, axis=0),\n",
    "    'Max': np.max(temporal_features_matrix, axis=0),\n",
    "    'Variance': np.var(temporal_features_matrix, axis=0)\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä Temporal Feature Statistics:\")\n",
    "print(feature_stats.round(4))\n",
    "\n",
    "# Feature correlation analysis\n",
    "feature_corr = np.corrcoef(temporal_features_matrix.T)\n",
    "feature_corr_df = pd.DataFrame(\n",
    "    feature_corr, \n",
    "    index=TEMPORAL_FEATURE_NAMES[:temporal_features_matrix.shape[1]],\n",
    "    columns=TEMPORAL_FEATURE_NAMES[:temporal_features_matrix.shape[1]]\n",
    ")\n",
    "\n",
    "print(f\"\\nüîó Highly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_corr_df)):\n",
    "    for j in range(i+1, len(feature_corr_df)):\n",
    "        corr_val = feature_corr_df.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                feature_corr_df.index[i], \n",
    "                feature_corr_df.columns[j], \n",
    "                corr_val\n",
    "            ))\n",
    "\n",
    "for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"   {feat1} ‚Üî {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Feature importance (variance-based)\n",
    "feature_importance = feature_stats['Variance'].values\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "axes[0, 0].barh(range(len(feature_importance)), feature_importance[sorted_indices])\n",
    "axes[0, 0].set_yticks(range(len(feature_importance)))\n",
    "axes[0, 0].set_yticklabels([TEMPORAL_FEATURE_NAMES[i] for i in sorted_indices], fontsize=8)\n",
    "axes[0, 0].set_xlabel('Variance (Feature Importance)')\n",
    "axes[0, 0].set_title('Temporal Feature Importance (Variance-based)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature correlation heatmap (top 10 features)\n",
    "top_features_idx = sorted_indices[:10]\n",
    "top_features_corr = feature_corr[np.ix_(top_features_idx, top_features_idx)]\n",
    "top_features_names = [TEMPORAL_FEATURE_NAMES[i] for i in top_features_idx]\n",
    "\n",
    "im = axes[0, 1].imshow(top_features_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0, 1].set_xticks(range(len(top_features_names)))\n",
    "axes[0, 1].set_yticks(range(len(top_features_names)))\n",
    "axes[0, 1].set_xticklabels(top_features_names, rotation=45, ha='right', fontsize=8)\n",
    "axes[0, 1].set_yticklabels(top_features_names, fontsize=8)\n",
    "axes[0, 1].set_title('Feature Correlation Matrix (Top 10)')\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "# Feature distribution (top 5 most important)\n",
    "top_5_features = temporal_features_matrix[:, sorted_indices[:5]]\n",
    "axes[1, 0].boxplot(top_5_features, labels=[TEMPORAL_FEATURE_NAMES[i] for i in sorted_indices[:5]])\n",
    "axes[1, 0].set_title('Distribution of Top 5 Features')\n",
    "axes[1, 0].set_ylabel('Feature Value')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(temporal_features_matrix)\n",
    "\n",
    "axes[1, 1].scatter(features_pca[:, 0], features_pca[:, 1], alpha=0.6)\n",
    "axes[1, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[1, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[1, 1].set_title('PCA of Temporal Features')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüîç PCA Analysis:\")\n",
    "print(f\"   PC1 explains {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"   PC2 explains {pca.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "print(f\"   Total explained: {sum(pca.explained_variance_ratio_):.1%}\")\n",
    "\n",
    "# Top contributing features to each PC\n",
    "feature_contributions_pc1 = pca.components_[0]\n",
    "feature_contributions_pc2 = pca.components_[1]\n",
    "\n",
    "print(f\"\\nüìä Top Contributing Features:\")\n",
    "print(\"PC1 (top 5):\")\n",
    "pc1_top = np.argsort(np.abs(feature_contributions_pc1))[-5:][::-1]\n",
    "for idx in pc1_top:\n",
    "    contrib = feature_contributions_pc1[idx]\n",
    "    print(f\"   {TEMPORAL_FEATURE_NAMES[idx]}: {contrib:.3f}\")\n",
    "\n",
    "print(\"PC2 (top 5):\")\n",
    "pc2_top = np.argsort(np.abs(feature_contributions_pc2))[-5:][::-1]\n",
    "for idx in pc2_top:\n",
    "    contrib = feature_contributions_pc2[idx]\n",
    "    print(f\"   {TEMPORAL_FEATURE_NAMES[idx]}: {contrib:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a6309",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive anomaly detection evaluation\n",
    "print(\"üéØ Evaluating Anomaly Detection Performance...\")\n",
    "\n",
    "# Create synthetic anomalies by adding noise to normal graphs\n",
    "def create_synthetic_anomaly(X, E, noise_level=0.3):\n",
    "    \"\"\"Create synthetic anomaly by adding noise to graph\"\"\"\n",
    "    X_anomaly = X.clone()\n",
    "    E_anomaly = E.clone()\n",
    "    \n",
    "    # Add noise to node features\n",
    "    if X.dtype == torch.float:\n",
    "        noise = torch.randn_like(X) * noise_level\n",
    "        X_anomaly = X + noise\n",
    "    else:\n",
    "        # For categorical data, randomly flip some categories\n",
    "        mask = torch.rand_like(X.float()) < noise_level\n",
    "        X_anomaly = torch.where(mask, 1 - X, X)\n",
    "    \n",
    "    # Add noise to edge features\n",
    "    if E.dtype == torch.float:\n",
    "        noise = torch.randn_like(E) * noise_level\n",
    "        E_anomaly = E + noise\n",
    "    else:\n",
    "        # Randomly flip some edges\n",
    "        mask = torch.rand_like(E.float()) < noise_level\n",
    "        E_anomaly = torch.where(mask, 1 - E, E)\n",
    "    \n",
    "    return X_anomaly, E_anomaly\n",
    "\n",
    "# Generate test set with known labels\n",
    "normal_samples = test_samples[:25]  # First 25 as normal\n",
    "anomaly_samples = []\n",
    "true_labels = []\n",
    "\n",
    "# Normal samples\n",
    "normal_errors = []\n",
    "for snapshot_path in normal_samples:\n",
    "    X = SnapshotLoader.get_X(snapshot_path)\n",
    "    E = SnapshotLoader.get_E(snapshot_path)\n",
    "    error = compute_single_graph_error(X, E, model, transition_matrices, CONFIG['mode'])\n",
    "    normal_errors.append(error)\n",
    "    true_labels.append(0)  # Normal\n",
    "\n",
    "# Synthetic anomaly samples\n",
    "anomaly_errors = []\n",
    "for snapshot_path in normal_samples:  # Use same graphs but add noise\n",
    "    X = SnapshotLoader.get_X(snapshot_path)\n",
    "    E = SnapshotLoader.get_E(snapshot_path)\n",
    "    \n",
    "    # Create anomaly version\n",
    "    X_anom, E_anom = create_synthetic_anomaly(X, E, noise_level=0.2)\n",
    "    error = compute_single_graph_error(X_anom, E_anom, model, transition_matrices, CONFIG['mode'])\n",
    "    anomaly_errors.append(error)\n",
    "    true_labels.append(1)  # Anomaly\n",
    "\n",
    "all_errors = normal_errors + anomaly_errors\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "print(f\"üìä Test Set Composition:\")\n",
    "print(f\"   Normal samples: {len(normal_errors)}\")\n",
    "print(f\"   Anomaly samples: {len(anomaly_errors)}\")\n",
    "print(f\"   Total samples: {len(all_errors)}\")\n",
    "\n",
    "# Evaluate performance with different thresholds\n",
    "thresholds_to_evaluate = [\n",
    "    ('percentile_95', threshold_vector['percentile_95_threshold']),\n",
    "    ('percentile_99', threshold_vector['percentile_99_threshold']),\n",
    "    ('z_score_2', threshold_vector['z_score_threshold_2']),\n",
    "    ('optimal', optimal_threshold)\n",
    "]\n",
    "\n",
    "performance_results = {}\n",
    "\n",
    "for thresh_name, thresh_value in thresholds_to_evaluate:\n",
    "    predicted_labels = (np.array(all_errors) > thresh_value).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tn = np.sum((predicted_labels == 0) & (true_labels == 0))\n",
    "    fp = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
    "    fn = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
    "    tp = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    performance_results[thresh_name] = {\n",
    "        'threshold': thresh_value,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': [[tn, fp], [fn, tp]]\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüèÜ Performance Results:\")\n",
    "print(f\"{'Threshold':<15} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Specificity':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for thresh_name, metrics in performance_results.items():\n",
    "    print(f\"{thresh_name:<15} {metrics['accuracy']:<10.3f} {metrics['precision']:<10.3f} \"\n",
    "          f\"{metrics['recall']:<10.3f} {metrics['f1_score']:<10.3f} {metrics['specificity']:<12.3f}\")\n",
    "\n",
    "# Find best performing threshold\n",
    "best_threshold = max(performance_results.keys(), key=lambda k: performance_results[k]['f1_score'])\n",
    "print(f\"\\nü•á Best performing threshold: {best_threshold} (F1: {performance_results[best_threshold]['f1_score']:.3f})\")\n",
    "\n",
    "# ROC Curve analysis\n",
    "if len(set(true_labels)) > 1:  # Ensure we have both classes\n",
    "    try:\n",
    "        auc_score = roc_auc_score(true_labels, all_errors)\n",
    "        print(f\"üìà AUC-ROC Score: {auc_score:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not compute AUC-ROC: {e}\")\n",
    "\n",
    "# Error distribution by class\n",
    "print(f\"\\nüìä Error Distribution by Class:\")\n",
    "print(f\"   Normal samples - Mean: {np.mean(normal_errors):.6f}, Std: {np.std(normal_errors):.6f}\")\n",
    "print(f\"   Anomaly samples - Mean: {np.mean(anomaly_errors):.6f}, Std: {np.std(anomaly_errors):.6f}\")\n",
    "print(f\"   Separation ratio: {np.mean(anomaly_errors) / np.mean(normal_errors):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b4859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly detection results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Error distribution by class\n",
    "axes[0, 0].hist(normal_errors, bins=20, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "axes[0, 0].hist(anomaly_errors, bins=20, alpha=0.7, label='Anomaly', color='red', density=True)\n",
    "axes[0, 0].axvline(performance_results['percentile_95']['threshold'], \n",
    "                   color='green', linestyle='--', label='95% Threshold')\n",
    "axes[0, 0].set_xlabel('Reconstruction Error')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Error Distribution by Class')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion matrix for best threshold\n",
    "best_cm = performance_results[best_threshold]['confusion_matrix']\n",
    "im = axes[0, 1].imshow(best_cm, interpolation='nearest', cmap='Blues')\n",
    "axes[0, 1].set_title(f'Confusion Matrix ({best_threshold})')\n",
    "axes[0, 1].set_ylabel('True Label')\n",
    "axes[0, 1].set_xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0, 1].text(j, i, str(best_cm[i][j]), \n",
    "                        ha=\"center\", va=\"center\", fontsize=12)\n",
    "\n",
    "axes[0, 1].set_xticks([0, 1])\n",
    "axes[0, 1].set_yticks([0, 1])\n",
    "axes[0, 1].set_xticklabels(['Normal', 'Anomaly'])\n",
    "axes[0, 1].set_yticklabels(['Normal', 'Anomaly'])\n",
    "\n",
    "# Performance comparison across thresholds\n",
    "metrics_names = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity']\n",
    "threshold_names = list(performance_results.keys())\n",
    "\n",
    "x = np.arange(len(threshold_names))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics_names):\n",
    "    values = [performance_results[t][metric] for t in threshold_names]\n",
    "    axes[1, 0].bar(x + i*width, values, width, label=metric.replace('_', ' ').title())\n",
    "\n",
    "axes[1, 0].set_xlabel('Threshold Strategy')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Performance Metrics Comparison')\n",
    "axes[1, 0].set_xticks(x + width * 2)\n",
    "axes[1, 0].set_xticklabels(threshold_names, rotation=45)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC-like curve\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "threshold_range = np.linspace(min(all_errors), max(all_errors), 100)\n",
    "\n",
    "for thresh in threshold_range:\n",
    "    predicted = (np.array(all_errors) > thresh).astype(int)\n",
    "    \n",
    "    tp = np.sum((predicted == 1) & (true_labels == 1))\n",
    "    fp = np.sum((predicted == 1) & (true_labels == 0))\n",
    "    tn = np.sum((predicted == 0) & (true_labels == 0))\n",
    "    fn = np.sum((predicted == 0) & (true_labels == 1))\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    tpr_values.append(tpr)\n",
    "    fpr_values.append(fpr)\n",
    "\n",
    "axes[1, 1].plot(fpr_values, tpr_values, 'b-', linewidth=2, label='ROC Curve')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random Classifier')\n",
    "\n",
    "# Mark performance points\n",
    "for thresh_name, metrics in performance_results.items():\n",
    "    if thresh_name in ['percentile_95', 'optimal']:\n",
    "        cm = metrics['confusion_matrix']\n",
    "        tp, fp, tn, fn = cm[1][1], cm[0][1], cm[0][0], cm[1][0]\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        axes[1, 1].plot(fpr, tpr, 'o', markersize=8, \n",
    "                        label=f'{thresh_name} (F1={metrics[\"f1_score\"]:.2f})')\n",
    "\n",
    "axes[1, 1].set_xlabel('False Positive Rate')\n",
    "axes[1, 1].set_ylabel('True Positive Rate')\n",
    "axes[1, 1].set_title('ROC Curve')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìã Summary Statistics:\")\n",
    "print(f\"   Baseline error rate (assuming 50% anomalies): 50.0%\")\n",
    "print(f\"   Best F1-score achieved: {performance_results[best_threshold]['f1_score']:.1%}\")\n",
    "print(f\"   Best accuracy achieved: {performance_results[best_threshold]['accuracy']:.1%}\")\n",
    "print(f\"   Error separation factor: {np.mean(anomaly_errors) / np.mean(normal_errors):.2f}x\")\n",
    "\n",
    "# Feature importance for anomaly detection (correlation with labels)\n",
    "if temporal_features_matrix.size > 0:\n",
    "    feature_anomaly_correlation = []\n",
    "    extended_labels = [0] * len(normal_samples) + [1] * len(normal_samples)  # Match temporal features\n",
    "    \n",
    "    if len(extended_labels) == temporal_features_matrix.shape[0]:\n",
    "        for i in range(temporal_features_matrix.shape[1]):\n",
    "            corr = np.corrcoef(temporal_features_matrix[:, i], extended_labels)[0, 1]\n",
    "            feature_anomaly_correlation.append(abs(corr))\n",
    "        \n",
    "        # Find most predictive features\n",
    "        top_predictive_indices = np.argsort(feature_anomaly_correlation)[-5:][::-1]\n",
    "        print(f\"\\nüîç Most Predictive Temporal Features for Anomaly Detection:\")\n",
    "        for idx in top_predictive_indices:\n",
    "            corr = np.corrcoef(temporal_features_matrix[:, idx], extended_labels)[0, 1]\n",
    "            print(f\"   {TEMPORAL_FEATURE_NAMES[idx]}: {abs(corr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac92c94",
   "metadata": {},
   "source": [
    "## 7. Model Comparison & Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf26790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare static vs dynamic mode performance (if both models available)\n",
    "print(\"üîÑ Comparing Static vs Dynamic Mode Performance...\")\n",
    "\n",
    "mode_comparison = {}\n",
    "\n",
    "# Test static mode if available\n",
    "try:\n",
    "    static_model, static_transitions, static_threshold = load_trained_model_and_threshold(\n",
    "        dataset=CONFIG['dataset'], \n",
    "        mode='static'\n",
    "    )\n",
    "    \n",
    "    # Compute errors with static model\n",
    "    static_errors = []\n",
    "    for snapshot_path in test_samples[:20]:  # Sample subset\n",
    "        X = SnapshotLoader.get_X(snapshot_path)\n",
    "        E = SnapshotLoader.get_E(snapshot_path)\n",
    "        error = compute_single_graph_error(X, E, static_model, static_transitions, 'static')\n",
    "        static_errors.append(error)\n",
    "    \n",
    "    mode_comparison['static'] = {\n",
    "        'mean_error': np.mean(static_errors),\n",
    "        'std_error': np.std(static_errors),\n",
    "        'threshold_95': static_threshold['percentile_95_threshold'],\n",
    "        'errors': static_errors\n",
    "    }\n",
    "    print(\"‚úÖ Static mode evaluation completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Static mode not available: {e}\")\n",
    "    mode_comparison['static'] = None\n",
    "\n",
    "# Dynamic mode (already loaded)\n",
    "dynamic_errors_subset = reconstruction_errors[:20]  # Match sample size\n",
    "mode_comparison['dynamic'] = {\n",
    "    'mean_error': np.mean(dynamic_errors_subset),\n",
    "    'std_error': np.std(dynamic_errors_subset),\n",
    "    'threshold_95': threshold_vector['percentile_95_threshold'],\n",
    "    'errors': dynamic_errors_subset\n",
    "}\n",
    "\n",
    "# Compare if both available\n",
    "if mode_comparison['static'] is not None:\n",
    "    print(f\"\\nüìä Static vs Dynamic Comparison:\")\n",
    "    print(f\"{'Metric':<20} {'Static':<15} {'Dynamic':<15} {'Improvement':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    static_mean = mode_comparison['static']['mean_error']\n",
    "    dynamic_mean = mode_comparison['dynamic']['mean_error']\n",
    "    mean_improvement = (dynamic_mean - static_mean) / static_mean * 100\n",
    "    \n",
    "    static_std = mode_comparison['static']['std_error']\n",
    "    dynamic_std = mode_comparison['dynamic']['std_error']\n",
    "    std_improvement = (dynamic_std - static_std) / static_std * 100\n",
    "    \n",
    "    print(f\"{'Mean Error':<20} {static_mean:<15.6f} {dynamic_mean:<15.6f} {mean_improvement:<15.2f}%\")\n",
    "    print(f\"{'Std Error':<20} {static_std:<15.6f} {dynamic_std:<15.6f} {std_improvement:<15.2f}%\")\n",
    "    \n",
    "    # Statistical significance test\n",
    "    from scipy.stats import ttest_ind\n",
    "    t_stat, p_value = ttest_ind(mode_comparison['static']['errors'], \n",
    "                                mode_comparison['dynamic']['errors'])\n",
    "    print(f\"\\nüß™ T-test Results:\")\n",
    "    print(f\"   T-statistic: {t_stat:.4f}\")\n",
    "    print(f\"   P-value: {p_value:.6f}\")\n",
    "    print(f\"   Difference is {'significant' if p_value < 0.05 else 'not significant'} (Œ±=0.05)\")\n",
    "\n",
    "# Baseline comparisons\n",
    "print(f\"\\nüìà Baseline Comparisons:\")\n",
    "\n",
    "# Simple statistical baselines\n",
    "baseline_results = {}\n",
    "\n",
    "# Z-score based anomaly detection\n",
    "z_scores = np.abs(stats.zscore(reconstruction_errors))\n",
    "z_threshold = 2.0  # Standard 2-sigma threshold\n",
    "z_predictions = z_scores > z_threshold\n",
    "z_anomaly_rate = np.mean(z_predictions)\n",
    "\n",
    "baseline_results['z_score'] = {\n",
    "    'method': 'Z-Score (2œÉ)',\n",
    "    'anomaly_rate': z_anomaly_rate,\n",
    "    'threshold': z_threshold\n",
    "}\n",
    "\n",
    "# IQR-based detection\n",
    "q1 = np.percentile(reconstruction_errors, 25)\n",
    "q3 = np.percentile(reconstruction_errors, 75)\n",
    "iqr = q3 - q1\n",
    "iqr_threshold = q3 + 1.5 * iqr\n",
    "iqr_predictions = reconstruction_errors > iqr_threshold\n",
    "iqr_anomaly_rate = np.mean(iqr_predictions)\n",
    "\n",
    "baseline_results['iqr'] = {\n",
    "    'method': 'IQR Outlier Detection',\n",
    "    'anomaly_rate': iqr_anomaly_rate,\n",
    "    'threshold': iqr_threshold\n",
    "}\n",
    "\n",
    "# Isolation Forest baseline (if sklearn available)\n",
    "try:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    # Reshape for sklearn\n",
    "    errors_reshaped = reconstruction_errors.reshape(-1, 1)\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    iso_predictions = iso_forest.fit_predict(errors_reshaped)\n",
    "    iso_anomaly_rate = np.mean(iso_predictions == -1)\n",
    "    \n",
    "    baseline_results['isolation_forest'] = {\n",
    "        'method': 'Isolation Forest',\n",
    "        'anomaly_rate': iso_anomaly_rate,\n",
    "        'threshold': 'Adaptive'\n",
    "    }\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è Isolation Forest not available (sklearn required)\")\n",
    "\n",
    "print(f\"{'Method':<25} {'Anomaly Rate':<15} {'Threshold':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for key, result in baseline_results.items():\n",
    "    threshold_str = f\"{result['threshold']:.4f}\" if isinstance(result['threshold'], (int, float)) else result['threshold']\n",
    "    print(f\"{result['method']:<25} {result['anomaly_rate']:<15.3%} {threshold_str:<15}\")\n",
    "\n",
    "# Feature ablation study\n",
    "print(f\"\\nüî¨ Feature Ablation Study:\")\n",
    "print(\"Analyzing impact of different feature groups...\")\n",
    "\n",
    "feature_groups = {\n",
    "    'temporal_basic': [0, 1],  # t_norm, seq_position\n",
    "    'edge_dynamics': [2, 3, 4, 5],  # birth, death, stability, intermittency\n",
    "    'structural_evolution': [6, 7, 8, 9, 10, 11],  # degree, density, neighborhood, clustering, triangles\n",
    "    'temporal_patterns': [12, 13],  # autocorr, periodicity\n",
    "    'centrality': [14, 15, 16],  # centrality evolution and persistence\n",
    "    'predictability': [17, 18, 19],  # entropy, surprise, edge predictability\n",
    "    'global_metrics': [20, 21]  # volatility, trend\n",
    "}\n",
    "\n",
    "if temporal_features_matrix.size > 0:\n",
    "    print(f\"Feature group importance (variance contribution):\")\n",
    "    for group_name, indices in feature_groups.items():\n",
    "        if max(indices) < temporal_features_matrix.shape[1]:\n",
    "            group_variance = np.mean([feature_stats.iloc[i]['Variance'] for i in indices])\n",
    "            print(f\"   {group_name:<20}: {group_variance:.6f}\")\n",
    "        else:\n",
    "            print(f\"   {group_name:<20}: Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3d2d5",
   "metadata": {},
   "source": [
    "## 8. Final Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "print(\"üìã COMPREHENSIVE EVALUATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model Performance Summary\n",
    "print(f\"\\nüéØ MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"   Mode: {CONFIG['mode']}\")\n",
    "print(f\"   Test samples evaluated: {len(reconstruction_errors)}\")\n",
    "print(f\"   Mean reconstruction error: {np.mean(reconstruction_errors):.6f}\")\n",
    "print(f\"   Error standard deviation: {np.std(reconstruction_errors):.6f}\")\n",
    "\n",
    "# Threshold Performance\n",
    "best_f1 = max(performance_results.values(), key=lambda x: x['f1_score'])['f1_score']\n",
    "print(f\"\\nüéØ THRESHOLD PERFORMANCE:\")\n",
    "print(f\"   Best F1-score achieved: {best_f1:.3f}\")\n",
    "print(f\"   Best threshold strategy: {best_threshold}\")\n",
    "print(f\"   Recommended threshold: {performance_results[best_threshold]['threshold']:.6f}\")\n",
    "\n",
    "# Feature Analysis\n",
    "if temporal_features_matrix.size > 0:\n",
    "    most_important_features = np.argsort(feature_stats['Variance'].values)[-5:][::-1]\n",
    "    print(f\"\\nüîç TEMPORAL FEATURE ANALYSIS:\")\n",
    "    print(f\"   Total features analyzed: {temporal_features_matrix.shape[1]}\")\n",
    "    print(f\"   Most important features:\")\n",
    "    for i, idx in enumerate(most_important_features, 1):\n",
    "        print(f\"     {i}. {TEMPORAL_FEATURE_NAMES[idx]} (variance: {feature_stats.iloc[idx]['Variance']:.4f})\")\n",
    "\n",
    "# Model Comparison\n",
    "if mode_comparison['static'] is not None:\n",
    "    improvement = ((mode_comparison['dynamic']['mean_error'] - mode_comparison['static']['mean_error']) \n",
    "                   / mode_comparison['static']['mean_error'] * 100)\n",
    "    print(f\"\\nüìà MODEL COMPARISON:\")\n",
    "    print(f\"   Dynamic vs Static improvement: {improvement:.2f}%\")\n",
    "    print(f\"   Statistical significance: {'Yes' if 'p_value' in locals() and p_value < 0.05 else 'Not tested'}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "\n",
    "# Threshold recommendations\n",
    "if best_f1 < 0.8:\n",
    "    print(f\"   ‚ö†Ô∏è  F1-score ({best_f1:.3f}) suggests room for improvement\")\n",
    "    print(f\"       Consider: More training data, feature engineering, or hyperparameter tuning\")\n",
    "\n",
    "if np.std(reconstruction_errors) / np.mean(reconstruction_errors) > 0.5:\n",
    "    print(f\"   ‚ö†Ô∏è  High error variance suggests inconsistent performance\")\n",
    "    print(f\"       Consider: Data preprocessing, outlier handling, or ensemble methods\")\n",
    "\n",
    "# Feature recommendations\n",
    "if temporal_features_matrix.size > 0:\n",
    "    high_corr_count = len([pair for pair in high_corr_pairs if abs(pair[2]) > 0.9])\n",
    "    if high_corr_count > 3:\n",
    "        print(f\"   üîß High feature correlation detected ({high_corr_count} pairs > 0.9)\")\n",
    "        print(f\"       Consider: Feature selection or dimensionality reduction\")\n",
    "\n",
    "print(f\"\\n‚úÖ EVALUATION COMPLETED\")\n",
    "print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'reconstruction_error_stats': {\n",
    "        'mean': float(np.mean(reconstruction_errors)),\n",
    "        'std': float(np.std(reconstruction_errors)),\n",
    "        'min': float(np.min(reconstruction_errors)),\n",
    "        'max': float(np.max(reconstruction_errors))\n",
    "    },\n",
    "    'best_threshold': {\n",
    "        'name': best_threshold,\n",
    "        'value': float(performance_results[best_threshold]['threshold']),\n",
    "        'f1_score': float(performance_results[best_threshold]['f1_score'])\n",
    "    },\n",
    "    'performance_metrics': {k: {key: float(val) if isinstance(val, (int, float, np.number)) else val \n",
    "                                for key, val in v.items() if key != 'confusion_matrix'} \n",
    "                            for k, v in performance_results.items()},\n",
    "    'sample_count': len(reconstruction_errors)\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "results_path = f\"evaluation_results_{CONFIG['dataset']}_{CONFIG['mode']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "try:\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=2)\n",
    "    print(f\"üìÅ Results saved to: {results_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save results: {e}\")\n",
    "\n",
    "# Final visualization - Executive Summary Dashboard\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('DiGress Anomaly Detection - Executive Summary Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Error distribution with threshold\n",
    "axes[0, 0].hist(reconstruction_errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(performance_results[best_threshold]['threshold'], color='red', \n",
    "                   linestyle='--', linewidth=2, label=f'Best Threshold')\n",
    "axes[0, 0].set_title('Reconstruction Error Distribution')\n",
    "axes[0, 0].set_xlabel('Error Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Performance metrics radar chart (simplified bar chart)\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']\n",
    "values = [performance_results[best_threshold][m.lower().replace('-', '_')] for m in metrics]\n",
    "bars = axes[0, 1].bar(metrics, values, color=['#FF9999', '#66B2FF', '#99FF99', '#FFD700', '#FF99CC'])\n",
    "axes[0, 1].set_title(f'Performance Metrics ({best_threshold})')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "for bar, value in zip(bars, values):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{value:.2f}', ha='center', va='bottom')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature importance (top 8)\n",
    "if temporal_features_matrix.size > 0:\n",
    "    top_8_indices = np.argsort(feature_stats['Variance'].values)[-8:]\n",
    "    top_8_names = [TEMPORAL_FEATURE_NAMES[i] for i in top_8_indices]\n",
    "    top_8_values = [feature_stats.iloc[i]['Variance'] for i in top_8_indices]\n",
    "    \n",
    "    axes[0, 2].barh(range(len(top_8_names)), top_8_values, color='lightcoral')\n",
    "    axes[0, 2].set_yticks(range(len(top_8_names)))\n",
    "    axes[0, 2].set_yticklabels(top_8_names, fontsize=8)\n",
    "    axes[0, 2].set_title('Top Temporal Features (by Variance)')\n",
    "    axes[0, 2].set_xlabel('Variance')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Threshold comparison\n",
    "threshold_names = list(performance_results.keys())\n",
    "f1_scores = [performance_results[t]['f1_score'] for t in threshold_names]\n",
    "axes[1, 0].bar(threshold_names, f1_scores, color='lightgreen')\n",
    "axes[1, 0].set_title('F1-Score by Threshold Strategy')\n",
    "axes[1, 0].set_ylabel('F1-Score')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Model comparison (if available)\n",
    "if mode_comparison['static'] is not None:\n",
    "    modes = ['Static', 'Dynamic']\n",
    "    mean_errors = [mode_comparison['static']['mean_error'], mode_comparison['dynamic']['mean_error']]\n",
    "    axes[1, 1].bar(modes, mean_errors, color=['orange', 'purple'])\n",
    "    axes[1, 1].set_title('Mean Reconstruction Error by Mode')\n",
    "    axes[1, 1].set_ylabel('Mean Error')\n",
    "    for i, v in enumerate(mean_errors):\n",
    "        axes[1, 1].text(i, v + max(mean_errors)*0.01, f'{v:.4f}', ha='center', va='bottom')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Static Mode\\nNot Available', ha='center', va='center', \n",
    "                    transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    axes[1, 1].set_title('Mode Comparison')\n",
    "\n",
    "# 6. Summary statistics\n",
    "summary_text = f\"\"\"\n",
    "Model Performance Summary\n",
    "\n",
    "Dataset: {CONFIG['dataset']}\n",
    "Mode: {CONFIG['mode']}\n",
    "Samples: {len(reconstruction_errors)}\n",
    "\n",
    "Best Threshold: {best_threshold}\n",
    "Best F1-Score: {best_f1:.3f}\n",
    "\n",
    "Error Statistics:\n",
    "  Mean: {np.mean(reconstruction_errors):.4f}\n",
    "  Std:  {np.std(reconstruction_errors):.4f}\n",
    "  \n",
    "Evaluation Date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, \n",
    "                fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "axes[1, 2].set_xlim(0, 1)\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéâ Evaluation framework completed successfully!\")\n",
    "print(f\"üíæ All results have been computed and visualized.\")\n",
    "print(f\"üìä Dashboard provides executive-level overview of model performance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
