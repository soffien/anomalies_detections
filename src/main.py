import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
import os
import sys
import glob
import torch.nn.functional as F
import numpy as np
from datetime import datetime
import random
import gc

# Imports des modules du projet
from models.denoising_network import DenoisingNetwork
from models.transition_matrices import DiGressTransitionMatrices
from models.graph_features import compute_graph_features
from models.graph_temporel_features import compute_temporal_graph_features
from models.visualize_snapshot import SnapshotLoader, get_snapshot_files
# residual block and layernorme a optimize
# D√©plac√© ici :
class SnapshotDataset(torch.utils.data.Dataset):
    def __init__(self, dataset_name, snapshot_files):
        self.dataset_name = dataset_name
        self.snapshot_files = snapshot_files
        
    def __len__(self):
        return len(self.snapshot_files)
        
    def __getitem__(self, idx):
        file_path = self.snapshot_files[idx]
        X = SnapshotLoader.get_X(file_path)
        E = SnapshotLoader.get_E(file_path)
        return X, E

#  NOUVEAU : Fonction de split al√©atoire
def split_snapshots_random(snapshot_files, train_ratio=0.8):
    """Split al√©atoire : m√©lange les snapshots"""
    snapshot_files_copy = snapshot_files.copy()  # √âviter de modifier l'original
    random.shuffle(snapshot_files_copy)
    
    total_snapshots = len(snapshot_files_copy)
    train_size = int(total_snapshots * train_ratio)
    
    train_files = snapshot_files_copy[:train_size]
    eval_files = snapshot_files_copy[train_size:]
    
    print(f"üìä Split al√©atoire:")
    print(f"  Training: {len(train_files)} snapshots")
    print(f"  √âvaluation: {len(eval_files)} snapshots")
    
    return train_files, eval_files

def compute_temporal_graph_features_simple(graph, current_t, num_timesteps, device):
    """Version simple : R√©p√®te le graphe pour simuler une s√©quence"""
    graph_sequence = [graph] * 3
    return compute_temporal_graph_features(graph_sequence, current_t, num_timesteps, device, window_size=3)

#  NOUVEAU : Fonction d'√©valuation int√©gr√©e avec optimisations m√©moire
def compute_threshold_vector(model_path, eval_snapshots, mode='dynamic'):
    """Calcule le vecteur de seuil sur les donn√©es d'√©valuation avec optimisations m√©moire"""
    print(f"üíæ Calcul du vecteur de seuil (mode: {mode}) avec optimisations m√©moire")
    
    # Configuration optimis√©e
    BATCH_SIZE = 2  # üî• R√âDUIT ENCORE PLUS pour l'√©valuation
    NUM_TIMESTEPS = 1000
    DEVICE = torch.device('cpu')
    
    # Vider le cache m√©moire
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    # Charger le mod√®le
    checkpoint = torch.load(model_path, map_location=DEVICE)
    
    # Matrices de transition
    transition_matrices = DiGressTransitionMatrices(
        num_timesteps=NUM_TIMESTEPS,
        dataset_name='uci'
    ).to(DEVICE)
    
    categories_info = transition_matrices.get_categories_info()
    num_node_types = categories_info['num_X_categories']
    num_edge_types = categories_info['num_E_categories']
    
    # Mod√®le
    model = DenoisingNetwork(
        num_node_classes=num_node_types,
        num_edge_classes=num_edge_types,
        hidden_dim=64,
        num_layers=3,
        mode=mode
    ).to(DEVICE)
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    
    # Dataset d'√©valuation avec batch size r√©duit
    eval_dataset = SnapshotDataset('uci', eval_snapshots)
    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=False)
    
    reconstruction_errors = []
    
    with torch.no_grad():
        for batch_idx, (batch_X, batch_E) in enumerate(eval_dataloader):
            print(f" üìä √âvaluation batch {batch_idx+1}/{len(eval_dataloader)}")
            
            batch_X = batch_X.to(DEVICE, non_blocking=True)
            batch_E = batch_E.to(DEVICE, non_blocking=True)
            
            t = torch.randint(1, NUM_TIMESTEPS + 1, (batch_X.size(0),), device=DEVICE)
            
            # Bruitage (m√™me processus que training mais avec autocast)
            with autocast():
                noisy_X_batch = []
                noisy_E_batch = []
                
                for b in range(batch_X.size(0)):
                    class TempGraph:
                        def __init__(self, X, E):
                            self.X = X
                            self.E = E
                    
                    graph = TempGraph(batch_X[b], batch_E[b])
                    noisy_graph = transition_matrices.apply_noise_to_graph(
                        graph, t[b].item(), DEVICE, graph.X.shape[0]
                    )
                    
                    noisy_X_batch.append(noisy_graph.X_onehot)
                    noisy_E_batch.append(noisy_graph.E_onehot)
                
                noisy_X = torch.stack(noisy_X_batch)
                noisy_E = torch.stack(noisy_E_batch)
                
                # Lib√©rer la m√©moire interm√©diaire
                del noisy_X_batch, noisy_E_batch
                gc.collect()
                
                # Features
                batch_features = []
                for i in range(batch_X.size(0)):
                    class FeatureGraph:
                        def __init__(self, X, E):
                            self.X = X
                            self.E = E
                    
                    feature_graph = FeatureGraph(noisy_X[i], noisy_E[i])
                    
                    if mode == "static":
                        features = compute_graph_features(
                            feature_graph, t[i], NUM_TIMESTEPS, DEVICE
                        )
                    else:
                        features = compute_temporal_graph_features_simple(
                            feature_graph, t[i], NUM_TIMESTEPS, DEVICE
                        )
                    
                    batch_features.append(features)
                
                batch_features = torch.stack(batch_features)
                
                #  UTILISER DIRECTEMENT LA LOSS DU MOD√àLE
                loss = model(noisy_X, noisy_E, batch_features)
                reconstruction_errors.append(loss.item())
                
                print(f"  Loss de reconstruction: {loss.item():.6f}")
            
            # Lib√©ration agressive de la m√©moire
            del batch_X, batch_E, noisy_X, noisy_E, batch_features, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
    
    # Calcul des statistiques de seuil
    if reconstruction_errors:
        errors = np.array(reconstruction_errors)
        threshold_vector = {
            'mean_error': np.mean(errors),
            'std_error': np.std(errors),
            'percentile_95_threshold': np.percentile(errors, 95),
            'percentile_99_threshold': np.percentile(errors, 99),
            'z_score_threshold_2': np.mean(errors) + 2 * np.std(errors),
            'sample_count': len(errors)
        }
        
        # Sauvegarder le vecteur de seuil
        os.makedirs("saved_models", exist_ok=True)
        threshold_path = f"saved_models/threshold_vector_{mode}_uci.pt"
        torch.save(threshold_vector, threshold_path)
        
        print(f" ‚úÖ Vecteur de seuil sauvegard√©: {threshold_path}")
        print(f" üéØ Seuil 95%: {threshold_vector['percentile_95_threshold']:.6f}")
        
        return threshold_vector
    else:
        print(" ‚ùå Aucune erreur calcul√©e")
        return None

def train(mode, dataset='uci', num_epochs=100, model_path=None, snapshot_files=None):
    """
    Entra√Ænement unifi√© avec optimisations m√©moire pour CPU
    
    Args:
        mode: "static" ou "dynamic"
        dataset: nom du dataset
        num_epochs: nombre d'√©poques
        model_path: chemin du mod√®le pr√©-entra√Æn√© (None pour partir de z√©ro)
        snapshot_files: liste des fichiers snapshot √† utiliser 
    """
    print(f"\n=== ENTRA√éNEMENT MODE {mode.upper()} AVEC OPTIMISATIONS M√âMOIRE CPU ===")
    
    # Configuration avec optimisations m√©moire pour CPU
    BATCH_SIZE = 4  # üî• R√âDUIT DE 32 √Ä 4 pour √©conomiser la RAM
    LEARNING_RATE = 0.001
    NUM_TIMESTEPS = 1000
    DEVICE = torch.device('cpu')  # CPU uniquement
    
    # üöÄ NOUVEAU: Mixed precision seulement si CUDA disponible
    use_mixed_precision = torch.cuda.is_available()
    scaler = GradScaler() if use_mixed_precision else None
    
    print(f"üíæ Configuration optimis√©e pour la m√©moire:")
    print(f"   Device: {DEVICE}")
    print(f"   Batch size r√©duit: {BATCH_SIZE}")
    print(f"   Mixed precision: {'Activ√© (CUDA)' if use_mixed_precision else 'D√©sactiv√© (CPU)'}")
    print(f"   Gradient accumulation: Activ√©")
    
    #  UTILISER LES SNAPSHOTS FOURNIS OU TOUS
    if snapshot_files is None:
        snapshot_files = get_snapshot_files(dataset)
    
    print(f"Nombre de snapshots pour training: {len(snapshot_files)}")
    
    # Vider le cache m√©moire
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    # Obtenir le nombre total de cat√©gories
    total_categories = SnapshotLoader.get_total_categories()
    print(f"Total categories: {total_categories}")
    
    # Afficher les distributions pour quelques snapshots seulement
    print("\n=== DISTRIBUTIONS (√©chantillon) ===")
    sample_files = snapshot_files[:min(2, len(snapshot_files))]  # R√©duit √† 2 √©chantillons
    for i, file_path in enumerate(sample_files):
        X = SnapshotLoader.get_X(file_path)
        E = SnapshotLoader.get_E(file_path)
        
        # Calculer les distributions pour ce snapshot
        X_counts = torch.zeros(6)
        E_counts = torch.zeros(2)
        
        # Distribution X
        for cat in range(total_categories):
            X_counts[cat] = (X[:, 1] == cat).sum().item()
        X_dist = X_counts / X_counts.sum()
        
        # Distribution E
        for cat in range(2):
            E_counts[cat] = (E == cat).sum().item()
        E_dist = E_counts / E_counts.sum()
        
        print(f"\nSnapshot {i}:")
        print(f"X distribution: {X_dist.tolist()}")
        print(f"E distribution: {E_dist.tolist()}")
        
        # Lib√©rer la m√©moire imm√©diatement
        del X, E, X_counts, E_counts
        gc.collect()
    
    # Cr√©er le dataset qui utilise SnapshotLoader
    dataset_obj = SnapshotDataset(dataset, snapshot_files)
    dataloader = DataLoader(dataset_obj, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)
    
    # Matrices de transition
    transition_matrices = DiGressTransitionMatrices(
        num_timesteps=NUM_TIMESTEPS,
        dataset_name='uci'
    ).to(DEVICE)
    
    categories_info = transition_matrices.get_categories_info()
    num_node_types = categories_info['num_X_categories']
    num_edge_types = categories_info['num_E_categories']
    
    # Mod√®le (garder en FP32 pour CPU)
    model = DenoisingNetwork(
        num_node_classes=num_node_types,
        num_edge_classes=num_edge_types,
        hidden_dim=64,
        num_layers=3,
        mode=mode
    ).to(DEVICE)
    
    # üöÄ CORRECTION: NE PAS convertir en half precision sur CPU
    print(f"üíæ Mod√®le configur√© en FP32 pour CPU")
    
    # Chargement mod√®le pr√©-entra√Æn√© si fourni
    if model_path:
        print(f"Chargement du mod√®le: {model_path}")
        checkpoint = torch.load(model_path, map_location=DEVICE)
        model_state_dict = model.state_dict()
        
        if 'model_state_dict' in checkpoint:
            pretrained_dict = checkpoint['model_state_dict']
        else:
            pretrained_dict = checkpoint
        
        # Copier tous les poids sauf feature_embedding
        for key in pretrained_dict:
            if 'feature_embedding' not in key:
                if key in model_state_dict:
                    model_state_dict[key] = pretrained_dict[key]
        
        model.load_state_dict(model_state_dict)
        print("Poids charg√©s")
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # üöÄ NOUVEAU: Gradient accumulation pour simuler des batch plus gros
    accumulation_steps = 8  # Simule batch_size * 8 = 32
    
    # Entra√Ænement
    best_loss = float('inf')
    
    for epoch in range(num_epochs):
        print(f"\n√âpoque {epoch+1}/{num_epochs}")
        
        model.train()
        total_loss = 0
        num_batches = 0
        
        # Reset de l'optimiseur pour l'accumulation
        optimizer.zero_grad()
        
        for batch_idx, (batch_X, batch_E) in enumerate(dataloader):
            print(f"\nüíæ Batch {batch_idx+1}/{len(dataloader)}")
            
            batch_X = batch_X.to(DEVICE, non_blocking=True)
            batch_E = batch_E.to(DEVICE, non_blocking=True)
            
            t = torch.randint(1, NUM_TIMESTEPS + 1, (batch_X.size(0),), device=DEVICE)
            
            # üöÄ CORRECTION: Forward pass avec ou sans autocast selon le device
            if use_mixed_precision and torch.cuda.is_available():
                context_manager = autocast('cuda')  # Utiliser la nouvelle API
            else:
                context_manager = torch.no_grad()  # Pas d'autocast sur CPU
                context_manager = torch.enable_grad()  # Mais on veut les gradients
            
            # Bruitage (processus simplifi√© pour √©viter les probl√®mes de dtype)
            noisy_X_batch = []
            noisy_E_batch = []
            
            for b in range(batch_X.size(0)):
                class TempGraph:
                    def __init__(self, X, E):
                        self.X = X
                        self.E = E
                
                graph = TempGraph(batch_X[b], batch_E[b])
                
                noisy_graph = transition_matrices.apply_noise_to_graph(
                    graph, t[b].item(), DEVICE, graph.X.shape[0]
                )
                
                noisy_X_batch.append(noisy_graph.X_onehot)
                noisy_E_batch.append(noisy_graph.E_onehot)
            
            noisy_X = torch.stack(noisy_X_batch)
            noisy_E = torch.stack(noisy_E_batch)
            
            # Lib√©rer la m√©moire interm√©diaire
            del noisy_X_batch, noisy_E_batch
            gc.collect()
            
            noisy_E_onehot = noisy_E
            
            # Features selon le mode
            batch_features = []
            for i in range(batch_X.size(0)):
                class FeatureGraph:
                    def __init__(self, X, E):
                        self.X = X
                        self.E = E
                
                feature_graph = FeatureGraph(noisy_X[i], noisy_E_onehot[i])
                
                if mode == "static":
                    features = compute_graph_features(
                        feature_graph, t[i], NUM_TIMESTEPS, DEVICE
                    )
                else:
                    features = compute_temporal_graph_features_simple(
                        feature_graph, t[i], NUM_TIMESTEPS, DEVICE
                    )
                
                batch_features.append(features)
            
            batch_features = torch.stack(batch_features)
            
            # Forward pass du mod√®le (sans autocast sur CPU)
            loss = model(noisy_X, noisy_E_onehot, batch_features)
            
            # üöÄ NOUVEAU: Normaliser la loss pour l'accumulation
            loss = loss / accumulation_steps
            
            # üöÄ CORRECTION: Backward pass avec ou sans gradient scaling
            if use_mixed_precision and scaler is not None:
                scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # üöÄ NOUVEAU: Gradient accumulation
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):
                # Mise √† jour des poids
                if use_mixed_precision and scaler is not None:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                    
                optimizer.zero_grad()
                print(f"   ‚úÖ Poids mis √† jour (accumulation: {accumulation_steps} steps)")
            
            total_loss += loss.item() * accumulation_steps  # D√©normaliser pour l'affichage
            num_batches += 1
            
            print(f"   Loss: {loss.item() * accumulation_steps:.4f}")
            
            # Lib√©ration agressive de la m√©moire
            del batch_X, batch_E, noisy_X, noisy_E, batch_features, loss
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            if batch_idx % 5 == 0:  # R√©duire la fr√©quence d'affichage
                print(f"\rBatch {batch_idx+1}/{len(dataloader)} | Loss: {total_loss/num_batches:.4f}", end="")
        
        avg_loss = total_loss / num_batches
        print(f"\n√âpoque {epoch+1} | Loss: {avg_loss:.4f}")
        
        # Cr√©er le dossier saved_models s'il n'existe pas
        os.makedirs("saved_models", exist_ok=True)
        
        # Sauvegarde du meilleur
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_model_path = f"saved_models/digress_{mode}_best_{dataset}.pt"
            torch.save(model.state_dict(), best_model_path)
            print(f"üíæ Nouveau meilleur mod√®le sauvegard√©: {best_loss:.4f}")
    
    # Sauvegarde finale
    final_path = f"saved_models/digress_{mode}_final_{dataset}.pt"
    torch.save({
        'model_state_dict': model.state_dict(),
        'final_loss': best_loss,
        'mode': mode,
        'config': {
            'dataset': dataset,
            'num_node_classes': num_node_types,
            'num_edge_classes': num_edge_types
        }
    }, final_path)
    
    print(f"\n‚úÖ Mod√®le sauvegard√©: {final_path}")
    print(f"üéØ Meilleure loss: {best_loss:.4f}")
    return final_path

def main():
    """Entra√Ænement bi-phas√© + √âvaluation avec optimisations m√©moire"""
    DATASET = 'uci'
    NUM_EPOCHS = 10  # üî• R√âDUIT POUR TESTS (passer √† 100 quand √ßa marche)
    
    print(f"üöÄ PIPELINE D'ENTRA√éNEMENT OPTIMIS√â POUR LA M√âMOIRE")
    print(f"   Dataset: {DATASET}")
    print(f"   √âpoques: {NUM_EPOCHS}")
    
    # Vider le cache au d√©but
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    #  Split des donn√©es
    all_snapshots = get_snapshot_files(DATASET)
    
    # üî• NOUVEAU: Limiter le nombre de snapshots pour les tests
    max_snapshots = 20  # Limiter √† 20 snapshots pour √©viter OOM
    if len(all_snapshots) > max_snapshots:
        print(f"‚ö†Ô∏è  Limitation des snapshots √† {max_snapshots} pour √©viter les probl√®mes de m√©moire")
        all_snapshots = all_snapshots[:max_snapshots]
    
    train_snapshots, eval_snapshots = split_snapshots_random(all_snapshots, train_ratio=0.8)
    
    # V√©rifier qu'on a des donn√©es
    if len(train_snapshots) == 0:
        print("‚ùå Aucun snapshot de training disponible!")
        return
    
    if len(eval_snapshots) == 0:
        print("‚ö†Ô∏è  Aucun snapshot d'√©valuation, utilisation d'un subset du training")
        eval_snapshots = train_snapshots[:2]  # Prendre 2 snapshots pour l'√©val
    
    # Phase 1: Static
    print("\nüèóÔ∏è  PHASE 1: STATIC")
    try:
        static_model_path = train(mode="static", dataset=DATASET, num_epochs=NUM_EPOCHS, snapshot_files=train_snapshots)
        print(f"‚úÖ Phase statique termin√©e: {static_model_path}")
        
        # Vider la m√©moire entre les phases
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
    except Exception as e:
        print(f"‚ùå Erreur en phase statique: {e}")
        return
    
    # Phase 2: Dynamic
    print("\nüîÑ PHASE 2: DYNAMIC")
    try:
        dynamic_model_path = train(mode="dynamic", dataset=DATASET, num_epochs=NUM_EPOCHS, model_path=static_model_path, snapshot_files=train_snapshots)
        print(f"‚úÖ Phase dynamique termin√©e: {dynamic_model_path}")
        
        # Vider la m√©moire avant l'√©valuation
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
    except Exception as e:
        print(f"‚ùå Erreur en phase dynamique: {e}")
        return
    
    #  Phase 3: √âvaluation
    print("\nüìä PHASE 3: √âVALUATION")
    try:
        threshold_vector = compute_threshold_vector(dynamic_model_path, eval_snapshots, mode="dynamic")
        
        print(f"\nüéâ PIPELINE COMPLET TERMIN√â:")
        print(f"   üìÅ Mod√®le dynamic: {dynamic_model_path}")
        if threshold_vector:
            print(f"   üéØ Seuil 95%: {threshold_vector['percentile_95_threshold']:.6f}")
            print(f"   üìä √âchantillons √©valu√©s: {threshold_vector['sample_count']}")
        else:
            print(f"   ‚ö†Ô∏è  √âvaluation incompl√®te")
            
    except Exception as e:
        print(f"‚ùå Erreur en phase d'√©valuation: {e}")
        print(f"   Le mod√®le est quand m√™me sauvegard√©: {dynamic_model_path}")

    # Nettoyage final
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    print(f"\nüíæ Nettoyage m√©moire termin√©")

if __name__ == "__main__":
    main()